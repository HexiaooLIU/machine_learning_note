1. 神经元激活函数必须是非线性的。如果使用线性函数作为激活函数，神经网络和线性回归没有什么区别了就。
2. 都是希望可以将连续值映射到01区间上，因此logistic回归和神经元会选择sigmoid函数。但作为神经元的激活函数不一定要选择sigmoid，只要是非线性函数就可以。
3.
-\frac{∂E_k}{∂v_{ih}}=-\frac{∂E_k}{∂b_h}\frac{∂b_h}{∂a_h}\frac{∂a_h}{∂v_{ih}}
\frac{∂a_h}{∂v_{ih}}=x_i
以及由5.15 e_h=-\frac{∂E_k}{∂b_h}\frac{∂b_h}{∂a_h}
即可得到更新公式eta_hx_i
4. 学习率太低，梯度下降慢，迭代次数多；学习率太好，梯度不收敛，在最小值附近震荡
5. 编码实现bp网络参考另一个repository bp网络
6. 在网络训练过程中，学习率不断改变，当误差减少时学习率增加，当误差增大时学习率减少。
when E(t) < E(t-1), eta(t+1) = (1+epsilon)*eta(t);
when E(t) > E(t-1), eta(t+1) = (1-epsilon)*eta(t)
7. 根据异或构造训练集 x1,x2作为输入，输出为y；根据ABP确定参数wi和bi,通过径向基函数计算误差函数偏导，对网络进行更新。。。
