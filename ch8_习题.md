1, $p-\delta=\frac12$, 则 $\delta=p-\frac12=\frac12-\epsilon$，
$k=\frac{n}2$, $P(H(n)\leqslant{\frac{n}2)=\sum_{i=0}^{\frac{n}2}p^i(1-p)^{n-i}\leqslant e^{-2(\frac12-\epsilon)^2n}}=e^{-\frac12(1-2\epsilon)^2n}$

4, GradientBoosting把上一个分类器的预测误差传给下一个分类器，以此生成新的分类器，并将梯度下降的下降方向作为权值。而Adaboost在训练的每一轮，根据样本分布为每个样本赋予权重用于下一个分类器的学习和生成权值。

6， Bagging主要关注降低方差。而对于朴素贝叶斯分类器，没有方差可以降低。对于朴素贝叶斯分类器不能用随机抽样的方法来提高泛化性能，因为使用全部训练样本生成的朴素贝叶斯分类器是最优分类器。

7, 随机森林每次从所有样本属性中随机抽几个属性用来计算，而Bagging每次训练使用全部属性，因此随机森林会速度更快

8, MultiBoosting可以有效降低误差和方差，但训练成本很大程度上增加；Iterative Bagging相比Bagging降低误差但相对方差上升。

9， 可以用k误差图对分类器多样性做可视化的度量。将每一对分类器作为二维空间上的一个点，x轴表示k度量，y轴表示两个分类器的平均误差。在图的左下角的点有低误差和低k值(高多样性)

10， 用Bagging提升kNN，每次训练随机抽样一个子样本集合训练分类器，对测试样本进行分类，最后取最多的一种分类。
