1,
相同：都是对探索和利用进行折中；
不同：UCB中，由于一开始 $n_k$值为零因此需要对每个摇臂先进行一次探索；Softmax方法中需要有初始参数

2，

---
输入：
- MDP四元组 $E=<X, A, P, R>$
- 被评估的策略 $\pi$

过程：
1. $\forall x \in X:V(x)=0$
2. loop
2. &emsp;&emsp; for t = 1, 2, ... do
3. &emsp;&emsp;&emsp;&emsp;$\forall x \in X:V'(x)=\sum_{a\in A}\pi(x,a)\sum_{x'\in X}P^a_{x\to x'}(R^a_{x\to x'}+\gamma V^{\pi}_{\gamma}(x'))$
4. &emsp;&emsp;&emsp;&emsp;if $max_{x\in X}|V(x)-V'(x)|<\theta$ :
5. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;break
6. &emsp;&emsp;&emsp;&emsp;else:
7. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$V=V'$
8. &emsp;&emsp;&emsp;&emsp;endif
9. &emsp;&emsp;endfor
10. endloop

输出：
- 状态值函数$V$
---

3,

---
输入：
- MDP四元组 $E=<X, A, P, R>$

过程：
1. $\forall x \in X:V(x)=0, \pi(x,a)=\frac{1}{|A(x)|}$
2. loop
2. &emsp;&emsp; for t = 1, 2, ... do
3. &emsp;&emsp;&emsp;&emsp;$\forall x \in X:V'(x)=\sum_{a\in A}\pi(x,a)\sum_{x'\in X}P^a_{x\to x'}(R^a_{x\to x'}+\gamma V^{\pi}_{\gamma}(x'))$
4. &emsp;&emsp;&emsp;&emsp;if $max_{x\in X}|V(x)-V'(x)|<\theta$ :
5. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;break
6. &emsp;&emsp;&emsp;&emsp;else:
7. &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$V=V'$
8. &emsp;&emsp;&emsp;&emsp;endif
9. &emsp;&emsp;endfor
10. &emsp;&emsp;$\forall x\in X:\pi'(x)=argmax_{a\in A}Q(x,a);$
11. &emsp;&emsp;if $\forall x :\pi'(x)=\pi(x)$ then
12. &emsp;&emsp;&emsp;&emsp;break
13. &emsp;&emsp;else
14. &emsp;&emsp;&emsp;&emsp;$\pi=\pi'$
15. &emsp;&emsp;endif
16. endloop

输出：
- 最优策略 $\pi$
---

4, 通过学习MDP模型获得的P和R不能保证精确度，对最重强化学习的结果也有很大影响； Model-free方法也需要先采样出一条轨迹在根据轨迹所涉及的所有s-a对值函数进行更新，学习效率也不高

5,
由式(16.30)
$$ Q^{\pi}(x,a)=\sum_{x' \in X}P^a_{x \rightarrow x'}(R^a_{x \rightarrow x'}+\gamma\sum_{a' \in A}\pi(x',a')Q^{\pi}(x',a')) $$
得到进行新一次采样得到的
$$Q(x',a')=R^a_{x\to x'}+\gamma Q_{\pi}(x',a')$$
即为式(16.29)中的 $r_{t+1}$ 项。由此可将式(16.29)变为
$$ Q^{\pi}_{t+1}(x,a)=Q^{\pi}_t(x,a)+\frac{1}{t+1}(R^a_{x\to x'}+\gamma Q_{\pi}(x',a')-Q^{\pi}_t(x,a)) $$
将 $\frac{1}{t+1}$ 替换成 $\alpha$ 即得到式(16.31)

6,

---
输入：
- 环境E;
- 动作空间E;
- 起始状态 $x_0$;
- 奖赏折扣 $\gamma$;
- 更新步长 $\alpha$;

过程：
1. $\theta=0$
2. $x=x_0,\alpha=\pi(x)=argmax_{a''}\theta^T(x;a'')$
3. for t = 1,2,... do
4. &emsp;&emsp; $r,x'=$ 在E种执行动作 $a=\pi^{\epsilon}(x)$ 产生的奖赏与转移的状态；
5. &emsp;&emsp;$a'=\pi(x')$;
6. &emsp;&emsp;$\theta=\theta+\alpha(r+\gamma\theta^T(x';a')-\theta^T(x;a))(x;a)$;
7. &emsp;&emsp;$\pi(x)=argmax_{a''}\theta^T(x;a'')$;
8. &emsp;&emsp;$x=x',a=a'$;
9. end for

输出：
- 策略 $\pi$
---
